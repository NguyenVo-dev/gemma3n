
# Refactored medical chatbot using Streamlit + Ollama chat API (gemma3n)
# File: medical_chatbot.py

import streamlit as st
import requests
import warnings
import time
import sys
import os

# Suppress warnings
warnings.filterwarnings("ignore")

st.set_page_config(
    page_title="Medical Chatbot with gemma3n",
    page_icon="üè•",
    layout="wide",
    initial_sidebar_state="expanded"
)

SYSTEM_PROMPT = """You are a medical information assistant using the gemma3n model. Follow these rules:

1. EMERGENCY SYMPTOMS: If patient mentions chest pain, difficulty breathing, severe bleeding, heart attack, stroke symptoms, loss of consciousness, severe burns, choking, poisoning, or allergic reaction:
   ‚Üí Respond: "üö® EMERGENCY: [symptom] detected! Call emergency services IMMEDIATELY (112 or 115). Do NOT wait for further instructions."

2. MEDICAL FORM REQUESTS: If patient mentions "form", "doctor", "contact doctor", "see doctor", "severe condition":
   ‚Üí Respond: "üìã Medical Form Available In The Menu. Fill out the form and it will be sent to one of our trusted doctors."

3. GENERAL SYMPTOMS: For other health concerns:
   ‚Üí Provide helpful medical information
   ‚Üí Suggest 2-3 possible causes
   ‚Üí Recommend consulting healthcare professionals
   ‚Üí Use empathetic language

4. GREETINGS: Respond warmly and ask how you can help.

Always end with: "Please consult a healthcare professional for proper diagnosis."
Never provide specific diagnoses or medication recommendations."""

CRITICAL_SYMPTOMS = [
    "chest pain", "difficulty breathing", "severe bleeding", "heart attack",
    "stroke symptoms", "loss of consciousness", "severe burns", "choking",
    "poisoning", "allergic reaction", "suicidal thoughts"
]


class OllamaClient:
    def __init__(self, base_url="http://localhost:11434"):
        self.base_url = base_url
        self.model = "gemma3n"   # √©p ch·ªâ d√πng gemma3n
        self.available = self._check_ollama()

    def _check_ollama(self):
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get('models', [])
                return any(self.model in model.get("name", "") for model in models)
            return False
        except:
            return False


class MedicalChatbot:
    def __init__(self):
        self.ollama = OllamaClient()
        self.model_info = self.ollama.model
        self.history = []

    def generate_response(self, user_input):
        user_input_lower = user_input.lower()
        for symptom in CRITICAL_SYMPTOMS:
            if symptom in user_input_lower:
                return f"üö® EMERGENCY: {symptom} detected!\n\nCall emergency services IMMEDIATELY (112/115)."

        if self.ollama.available:
            messages = [{"role": "system", "content": SYSTEM_PROMPT}]
            for turn in self.history:
                messages.append({"role": "user", "content": turn["user"]})
                messages.append({"role": "assistant", "content": turn["assistant"]})
            messages.append({"role": "user", "content": user_input})

            response = self.ollama.generate_response(messages)
            if response:
                self.history.append({"user": user_input, "assistant": response})
                return f"{response}\n\nü§ñ *Response generated by {self.model_info} via Ollama*"

        return self._fallback_response(user_input)

    def _fallback_response(self, user_input):
        return f"""Thank you for your question about: "{user_input}"

Please consult a healthcare professional for proper diagnosis.

‚ö†Ô∏è *Fallback response - Ollama/gemma3n not available.*"""

    def reset_history(self):
        self.history = []

    def get_status(self):
        return {
            "ollama_available": self.ollama.available,
            "model": self.model_info,
            "conversation_turns": len(self.history),
            "mode": "AI" if self.ollama.available else "Fallback"
        }


def main():
    bot = MedicalChatbot()

    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
        status = bot.get_status()
        msg = f"‚úÖ Connected to {status['model']}" if status['ollama_available'] else "‚ö†Ô∏è Ollama not available, using fallback mode."
        st.session_state.chat_history.append({"role": "assistant", "content": msg})

    st.sidebar.title("üè• Medical Chatbot")
    status = bot.get_status()
    st.sidebar.write(f"**Mode:** {status['mode']}")
    st.sidebar.write(f"**Model:** {status['model']}")
    st.sidebar.write(f"**Turns:** {status['conversation_turns']}")
    if st.sidebar.button("üîÑ Reset Chat"):
        bot.reset_history()
        st.session_state.chat_history = []
        st.rerun()

    st.title("ü§ñ Chat with Medical Assistant")

    for msg in st.session_state.chat_history:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    prompt = st.chat_input("Enter your symptoms or question")
    if prompt:
        st.session_state.chat_history.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        with st.chat_message("assistant"):
            with st.spinner("Generating response..."):
                response = bot.generate_response(prompt)
                st.session_state.chat_history.append({"role": "assistant", "content": response})
                st.markdown(response)


if __name__ == "__main__":
    main()
